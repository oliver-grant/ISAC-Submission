\documentclass[a4paper,UKenglish]{lipics-v2016}
%This is a template for producing LIPIcs articles. 
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"
 
\usepackage{microtype}%if unwanted, comment out or use option "draft"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{A Sample Article for the LIPIcs series\footnote{This work was partially supported by someone.}}
\titlerunning{A Sample LIPIcs Article} %optional, in case that the title is too long; the running title should fit into the top page column

%% Please provide for each author the \author and \affil macro, even when authors have the same affiliation, i.e. for each author there needs to be the  \author and \affil macros
\author[1]{Oliver D. L. Grant}
\author[2]{J. Ian Munro}
\affil[1]{University of Waterloo, Waterloo, Canada\\
  \texttt{odlgrant@uwaterloo.ca}}
\affil[2]{University of Waterloo, Waterloo, Canada\\
  \texttt{imunro@uwaterloo.ca}}
\authorrunning{O.\,D.\,L. Grant and J.\,I. Munro} %mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Oliver D. L. Grant and J. Ian Munro}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation". 
\keywords{Dummy keyword -- please provide 1--5 keywords}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Acces}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
We examine optimal and near optimal solutions to the classic binary search tree problem of Knuth. We are given a set of $n$ keys $B_1, B_2, ..., B_n$ and $2n+1$ frequencies. ${p_1, p_2, ..., p_n}$ represent the probabilities of searching for each given key, and ${q_0, q_1, ..., q_n}$ represent the probabilities of searching in the gaps between and outside of these keys. We have that $\sum\limits_{i=0}^n q_i + \sum\limits_{i=1}^n p_i = 1$. First, we re-examine an approximate solution of G{\"u}ttler, Melhorn and Schneider which was shown to have a worst case bound of $c \cdot H + 2$ where $c \geq \frac{1}{H(\frac{1}{3},\frac{2}{3})} \approx 1.08$, and $H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})$ is the entropy of the distribution. We give an improved worst case bound on the heuristic of $H+4$. Next, we examine the optimum binary search tree problem under a model of external memory. We use the Hierarchical Memory Model of Aggarwal et al. The model has an unlimited number of registers, $R_1, R_2, ...$ each with its own location in memory (a positive integer). We have a set of memory sizes $m_1, m_2, ..., m_l$ which are monotonically increasing. Each memory level has a finite size except $m_l$ which we assume has infinite size. Each memory level has an associated cost of access $c_1, c_2, ..., c_l$. We assume that $c_1 < c_2 < ... < c_l$. We propose an approximate solutions which run in $O(n)$ time where $n$ is the number of words in our data set. We also examine the related problem of binary trees on multisets of probabilities where keys are unordered and we do not differentiate between which probabilities must be leaves, and which must be internal nodes. We provide a simple $O(n \lg(n))$ algorithm that is within an additive $\frac{n+1}{2n}$ of optimal on a multiset of $n$ keys. 
 \end{abstract}

\section{Introduction and Background}

In this chapter we provide an introduction to binary search trees and the optimum binary search tree problem. We also give motivation for studying binary search trees and give an overview of the work presented in this thesis. We note that for the entirety of this work, we will use $\lg$ to represent $log_2$.

Knuth first proposed the optimum binary search tree problem in 1971 \cite{knuth1971optimum}. We are given a set of $n$ keys (originally known as words), $B_1, B_2, ..., B_n$ and $2n+1$ frequencies. ${p_1, p_2, ..., p_n}$ represent the probabilities of searching for each given key, and ${q_0, q_1, ..., q_n}$ represent the probabilities of searching in the gaps between and outside of these keys. We have that
\begin{align*}
\sum\limits_{i=0}^n q_i + \sum\limits_{i=1}^n p_i = 1
 \end{align*}
We also assume without loss of generality that $q_{i-1}+p_i+q_i \neq 0$ for any $i \in \{1,...,n\}$. Otherwise, we could simply solve the problem with key $p_i$ removed. The keys must make up the internal nodes of the tree while the gaps make up the leaves. Our goal is to construct a binary search tree such that expected cost of search is minimized. This expected cost of search is also sometimes referred to as the expected path length. It is formally defined as: 
\begin{equation}\label{1.1}
P = \sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{j=0}^{n} q_j \cdot(d_T(B_j, B_{j+1}))
\end{equation}
where $p_i$ and $q_j$ are the probabilities of searching for key $B_i$ or gap $(B_{i-1}, B_i)$ respectively and $d_T(B_i)$ and $d_T(B_j, B_{j+1})$ are the depths of the internal node for $B_i$ and the leaf for $(B_j, B_{j-1})$ respectively in the tree $T$. Note that we assume that $B_0$ represents $- \infty$ and $B_{n+1}$ represents $\infty$. Note that we charge $1$ extra to search for a key at depth $l$ than a leaf at depth $l$ because it requires an extra operation to confirm an internal node, whereas we do not need this confirmation if the node is a leaf. The optimal solution of Knuth uses dynamic programming and requires $\Theta(n^2)$ time, and $\Theta(n^2)$ space \cite{knuth1971optimum}. This solution is both time and space intensive. We will later examine an approximate solution to this problem of G{\"u}ttler, Mehlhorn and Schneider (the Modified Entropy Rule) which uses $O(n^2)$ time but $O(n)$ space \cite{guttler1980binary}. We will improve its worst-case expected search cost bound. While all of the aforementioned algorithms examine the problem in the RAM model, we will also examine the problem in more realistic models of memory and look at approximate solutions under these settings.

While modern computers typically only support two-way branching, the optimum binary search tree (BST) problem proposed by Knuth uses the three-way branch model. This model allows a single comparison operation to transfer control to three different locations. \\~\\ Examples of this can be seen in FORTRAN IV which describes the arithmetic IF \cite{Dock:228063}:

\definecolor{lightgrey}{rgb}{0.95,0.92,0.92} % Defines the color used for content box headers
\colorbox{lightgrey}{ \fontfamily{cmtt}\selectfont \uppercase{IF (expr) label1, label2, label3} } 
 
\noindent Control is transferred to \uppercase{label1}, \uppercase{label2} or \uppercase{label3} if $expr < 0$, $expr=0$, or $expr > 0$ respectively using a single comparison command. While modern programming languages scarcely use this arithmetic IF, and compilers may simply encode such expressions using multiple logical if statements, many machines in in the FORTRAN IV era did. For example, the ARM instruction set would utilize condition codes based on comparison operations which could express negative, zero, or positive values. The condition codes would then be examined to determine control flow \cite{ARM}.

In 1971, C. Gotlieb and Walker gave an approximate solution to the optimum binary search tree problem \cite{walker1971top}. Knuth shortly thereafter gave the first optimal solution \cite{knuth1971optimum}. Knuth's optimal solution requires $O(n^2)$ time and space which is too costly in many situations. Several others have since examined the approximate version of the problem. While unable to bound an approximate algorithm within a constant of the optimal solution, many authors have been able to bound the cost based on the entropy of the distribution of probabilities, $H$. Specifically, 
\begin{align*}
H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j}).
\end{align*}
In 1975, P. Bayer showed that 
\begin{align*}
H-\lg H-(\lg e-1) \leq C_{Opt} \leq C_{WB}, C_{MM} \leq H + 2
\end{align*}
where $C_{Opt}, C_{WB}$, and $C_{MM}$ are costs for the optimal solution, as well as weight-balanced method of Knuth \cite{knuth1971optimum} and min-max heuristic of P. Bayer \cite{bayer1975improved}. Weight-balanced and min-max cost heuristics are greedy and require both $O(n)$ time and $O(n)$ space to run with the $O(n)$ implementations due to Fredman \cite{fredman1975two}. These greedy heuristics use a top-down approach where the tree root is selected from among the $n$ keys, and we recurse in both the left and right subtrees. Let $P_L(B_i)$ and $P_R(B_i)$ represent the probabilities of searching for a key before or after key $B_i$ respectively. The Weight-balanced approach, makes this greedy root selection by picking the root $B_i$ such that $|P_L(B_i)-P_R(B_i)|$ is minimized. The min-max heuristic selects the root $B_i$ such with minimum $\max(P_L(B_i), P_R(B_i))$.
  In 1980, G{\"u}ttler, Mehlhorn and Schneider presented a new heuristic, the modified entropy rule \cite{guttler1980binary} which built upon the ideas of Horibe \cite{horibe1977improved}. The Entropy Rule greedily selects $B_i$ as the root such that
\begin{align*}
H(P_L(B_i), p_i, P_R(B_i)) = P_L(B_i)\cdot\lg(\frac{1}{P_L(B_i)}) + p_i \cdot \lg(\frac{1}{p_i}) + P_R(B_i)\cdot\lg(\frac{1}{P_R(B_i)})  
\end{align*}
\noindent is maximized. As discussed in Chapter~\ref{An Improved Bound for the Modified Minimum Entropy Heuristic}, this was modified to improve its performance. G{\"u}ttler, Mehlhorn and Schneider gave empirical evidence that the modified heuristic out-performed others \cite{guttler1980binary}. While the heuristic took $O(n^2)$ time, it only required $O(n)$ space, a huge savings over the optimal solution. However, they were unable to prove that the cost of the modified entropy rule $C_{ME} \leq H+2$ (unlike previous weight-balanced and min-max heuristics) and settled with $C_{ME} \leq c_1\cdot H+2$ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$. We re-examine this method and provide a new bound of $H+4$ in Chapter~\ref{An Improved Bound for the Modified Minimum Entropy Heuristic}. In 1993, De Prisco and De Santis presented a new heuristic for constructing a near-optimum binary search tree \cite{de1993binary}. The method is discussed in more detail in section~\ref{sec:deBST} and has an upper bounded cost of at most $H+1-q_0-q_n+q_{max}$ where $q_{max}$ is the maximum weight leaf node. This method was later updated by Bose and Dou\"{i}eb (and is also discussed in section~\ref{sec:deBST}) to have a worst case cost of \cite{bose2009efficient}
\begin{align*}
H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}.
\end{align*}
Here, $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$ where $P$ is the number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves (gaps) and,  $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and gaps except $q_0$ and $q_n$.

In Chapter~\ref{An Improved Bound for the Modified Minimum Entropy Heuristic}, we re-examine the modified entropy rule of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}. This is an $\Theta(n^2)$ time, $\Theta(n)$ space, algorithm for approximating the optimum binary search tree problem in the RAM model. The method works very well in practice, and the group had great experimental results, but unfortunately they could not bound the worst case expected cost as well as they would have hoped. While simpler solutions like the \textit{Min-max} of P. Bayer \cite{bayer1975improved} and \textit{Weight Balanced} technique of Knuth \cite{knuth1971optimum} have worst case costs of at most $H+2$, the trio's modified entropy technique was only shown to have a worst case expected search cost of at most $c\cdot H+2$ where $c \approx 1.08$ \cite{bayer1975improved, guttler1980binary}. We provide a new argument of the modified entropy rule's worst case expected search cost and show that it is within an additive factor of entropy: at worst $H+4$. In Chapter~\ref{Approximate Binary Search in the Hierarchical Memory Model}, we move on to external memory models, examining the optimum binary search tree problem under the Hierarchical Memory Model of Aggarwal et al. \cite{aggarwal1987model}. We provide two algorithms which run in $O(n)$ time and bound their worst case expected costs. We show that the solutions provided both give a direct improvement over a solution of Thite provided under the related HMM$_2$ model \cite{thite2008optimum}. In Chapter~\ref{BST over Multisets}, we consider a variant of the optimum binary search tree problem (in the RAM model) where the set of probabilities given are from an unordered multiset. We show that for a multiset with $n$ probabilities, a simple greedy algorithm is within $\frac{n+1}{2n}$ of optimal.

\section{An Improved Bound for the Modified Minimum Entropy Heuristic}

We show that the Modified Minimum Entropy Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary} is within an additive factor of entropy: at worst $H+4$. The previous bound was best upper bound was $c_1\cdot H+2$ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$.

Recall equation \ref{1.1}, $H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})$. We also use
\begin{align*}
H(x_1,x_2,...,x_n) = \sum_{i=1}^{n} x_i\cdot\lg(\frac{1}{x_i})
\end{align*} to describe the entropy of any probability distribution $(x_1, x_2, ...., x_n)$. For subtree $t$, we let 
\begin{align*}
p_t=\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i
\end{align*}
 be its total probability (the sum of the probability of all nodes within the subtree). $P_{L}(B_i)$ and $P_{R}(B_i)$ are probabilities of searching lexicographically before or after (respectively) key $B_i$. $P_{L}(B_i, B_{i+1})$ and $P_{R}(B_i, B_{i+1})$ are probabilities of searching lexicographically before (or equal to) $B_i$ and after (or equal to) $B_{i+1}$ respectively. For a subtree $t$, $P^t_L(B_i)$ and $P^t_R(B_i)$ describe the normalized probabilities of searching for a key to the left or right of $B_i$ within $t$, and $P^t_L(B_i,B_{i+1})$ and $P^t_R(B_i,B_{i+1})$ have analogous definitions. 
We let  
\begin{equation}
E_t=H(P^t_L(B_i), \frac{p_i}{p_t}, P^t_R(B_i))
\end{equation} be the local entropy of a subtree $t$ rooted at key $B_i$. 

The entropy rule originally by Horibe \cite{horibe1977improved} for greedy root selection then explain how it was modified in \cite{guttler1980binary}. For a subtree $t$ with probability $p_t$, the entropy rule greedily chooses the key $B_i$ as the root such that $H(P^t_L(B_i), \frac{p_i}{p_t}, P^t_R(B_i))$ is maximized. While this rule behaves quite well in practice, certain cases cause it to have poor performance (refer to Figure 3.1). 

Figure 3.1 demonstrates the shortcomings of the entropy rule heuristic. Given the probability set $\{q_0 = \frac{1}{5}, p_1 = \frac{1}{5}, q_1 = 0, p_2 = 0, q_3 = \frac{3}{5}\}$ the entropy rule will mistakenly choose key $B_1$ as the root while selecting $B_2$ as the root produces a better tree. This mistake is remedied in the modified entropy rule of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}. The modified entropy heuristic chooses the root in one of the following three ways:

\begin{itemize}
\item If there exists key $B_i$ such that $\frac{p_i}{p_t} > \max(P^t_L(B_i), P^t_R(B_i))$ we always select $B_i$ as the root.

\item If there exists a gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P^t_L(B_i, B_{i+1}), P^t_R(B_i, B_{i+1}))$ then we select the root from among $B_i$ and $B_{i+1}$. $B_i$ is chosen if $P^t_L(B_i, B_{i+1}) > P^t_R(B_i, B_{i+1})$ and $B_{i+1}$ is chosen otherwise.

\item Otherwise, $B_i$ is selected such that $H(P^t_L(B_i), \frac{p_i}{p_t}, P^t_R(B_i))$ is maximized (as in the original entropy rule).

\end{itemize}

The approach proposed by G{\"u}ttler, Mehlhorn and Schneider takes $O(n^2)$ time in the worst case and $O(n)$ space.

\begin{lemma}\label{entr2x}
If $x \leq \frac{1}{2}$ then $H(x, 1-x) \geq 2x$.
\end{lemma}
\begin{proof}
We refer the reader to Gallager's 1968 work \cite{gallager1968information}.
\end{proof}

Next, we describe a Lemma which breaks our choice of root in the greedy modified entropy heuristic into one of three cases (not to be confused with the three \textit{rules} used in section~\ref{The Modified Entropy Rule}). 

\begin{lemma}\label{MECases}
When using the modified entropy rule chooses the root $B_r$ of a subtree $t$ with total probability $p_t$, one of the following three cases must occur: 
\begin{align*}
&\text{Case 1) } E_t \geq 1-2 \frac{p_r}{p_t} \\
&\text{Case 2) There exists gap}  (B_i, B_{i+1}) \text{ such that } \frac{q_i}{p_t} > max(P^t_L(B_i, B_{i+1}), P^t_R(B_i, B_{i+1}))\\
&\text{Case 3) }  \max(P^t_L(B_r), P^t_R(B_r)) < \frac{4}{5}
\end{align*}

\end{lemma}
\begin{proof} At a high level, we first show that \textit{Rule a)} from section~\ref{The Modified Entropy Rule} implies \textit{Case 1}. We also show that if there exists $B_i$ such that $P^t_L(B_i) \leq \frac{1}{2}$ and $P^t_R(B_i) \leq \frac{1}{2}$, but cannot apply \textit{Rule a)}, then we still have \textit{Case 1}. Assuming that neither of the two aforementioned conditions occur, we must have that there exists some gap $(B_i, B_{i+1})$ spanning the middle of the data set. Given this condition, we show that if \textit{Case 2} does not occur (i.e. we cannot use \textit{Rule b)} of section~\ref{The Modified Entropy Rule}) then \textit{Case 3} must occur, completing the proof. 


\noindent\textbf{\textit{Rule a) $\implies$ Case 1}} \\
 First, suppose there exists some $p_i$ such that $\frac{p_i}{p_t} > \max(P^t_L(B_i), P^t_R(B_i))$. By the \textit{Rule a)} of section~\ref{The Modified Entropy Rule}, it must be selected as the root and thus $r=i$. Moreover, both $P^t_L(B_i)$ and $P^t_R(B_i))$ must be less than one half. Thus, using Lemma~\ref{entr2x} we have: 
\begin{align*}
E_t &\geq H( max(P^t_L(p_i), P^t_R(p_i)), 1-max(P^t_L(p_i), P^t_R(p_i)) \\
 &\geq 2\cdot \max(P^t_L(p_i), P^t_R(p_i)) \\  &\geq 1-\frac{p_i}{p_t} \\ 
 &\geq 1-2 \frac{p_i}{p_t} = 1-2 \frac{p_r}{p_t} 
\end{align*}
 as required. \\
 
\noindent\textbf{\textit{$B_i$ spans middle $\implies$ Case 1}} \\
If we do not have some $p_i$ such that $\frac{p_i}{p_t} > \max(P^t_L(B_i), P^t_R(B_i))$ but do have some $B_i$ such that $P^t_L(B_i) \leq \frac{1}{2}$ and $P^t_R(B_i) \leq \frac{1}{2}$ then we must use \textit{Rule c)} of section~\ref{The Modified Entropy Rule}. We then must have that:
\begin{align*}
 E_t &\geq H(P^t_L(B_i), \frac{B_i}{p_t} , P^t_R(B_i)) \text{ and} \\
 0 &\leq P^t_L(B_i) \leq 0.5 \text{ and}\\
 0 &\leq \frac{p_i}{p_t} \leq 0.5 \text{ and}\\
 0 &\leq P^t_R(B_i) \leq 0.5
\end{align*} 
   then we know that \\
\begin{align*}
H(P^t_L(p_i), \frac{p_i}{p_t} , P^t_R(p_i)) \geq H(1/2, 1/2) = 1
\end{align*}
in this case.
 Thus, combining the above two cases, if have some $B_i$ such that $P^t_L(B_i) \leq \frac{1}{2}$ and $P^t_R(B_i) \leq \frac{1}{2}$ then $E_t \geq 1-2p_r$ as required. \\

\noindent\textbf{\textit{$\big($NOT($B_i$ spans mid) AND NOT(Case 1) AND NOT(Case 2)$\big)$ $\implies$ Case 3}} \\
Otherwise, we must have some gap $(B_i, B_{i+1})$ spanning the middle of the data set (i.e. $P^t_L(B_i, B_{i+1}) < \frac{1}{2}$ and $P^t_R(B_i, B_{i+1}) < \frac{1}{2}$). Suppose that \textit{Case 2} does not occur (i.e. we cannot use \textit{Rule b}): there does not exist a $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P^t_L(B_i, B_{i+1}), P^t_R(B_i, B_{i+1}))$. Then, for any root $r$ of $t$ we have that \\
\begin{align*}
\max(P^t_L(B_r), P^t_R(B_r)) &\geq \min(P^t_L(B_r), P^t_R(B_r))\\ 
\max(P^t_L(B_r), P^t_R(B_r)) &\geq q_i
\end{align*}
Thus, for any root $r$:
\begin{align*}
\max(P^t_L(p_r), P^t_R(p_r)) \geq 1/3
\end{align*}
 and by our assumption 
\begin{align*}
\max(P^t_L(p_r), P^t_R(p_r)) < \frac{1}{2}.
\end{align*}
 So, as in the proof of table 3 (5.3) in \cite{guttler1980binary}\\
\begin{align*}
E_t \geq H(1/3, 2/3) \approx 0.92.
\end{align*}
Either \textit{Case 1} occurs, or we have that:
\begin{align*}
E_t &< 1-2\frac{p_r}{p_t} \\
\implies \frac{p_r}{p_t} &< \frac{1-H(\frac{1}{3}, \frac{2}{3})}{2} \approx 0.04.
\end{align*}

Suppose for contradiction that $\max(P^t_L(p_r), P^t_R(p_r)) \geq \frac{4}{5} p_t$ then we have:
\begin{align*}
E_t \leq H(\frac{4}{5}, \frac{1-H(\frac{1}{3}, \frac{2}{3})}{2}, \frac{1}{5}-\frac{1-H(\frac{1}{3}, \frac{2}{3})}{2}) \approx 0.87 < 0.92 \approx H(\frac{1}{3}, \frac{2}{3}) \leq E_t
\end{align*}
which is a contradiction.
Thus, if we do not have \textit{Case 1} or \textit{Case 2} we must have \textit{Case 3} which completes the proof.

\end{proof}

Before we examine the main theorem we need a small claim (proven in the appendix TODO).

\newtheorem{claim}{Claim}
\begin{claim}\label{Claim1}
$H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) \geq 1- \frac{4}{5} x^2$ when $0 < x < \frac{1}{2}$
\end{claim}

\begin{theorem}
Let $C_{ME}$ be the expected cost of search for a tree made by the modified entropy rule. Then $C_{ME} \leq H + 4$
\end{theorem}

\begin{proof}
This uses a similar style to the proof of Theorem 4.4 in \cite{bayer1975improved}.
We bind each $E_t$ for each subtree of our BST on a case by case basis using the cases of Lemma~\ref{MECases}.\\
If \textit{Case 1} occurs, we obviously have that
\begin{equation}\label{EQC1}
E_t \geq 1-2 \frac{p_r}{p_t}.
\end{equation}
Note that this can only happen once for each key (a key can only be root once). \\

As mentioned in Lemma~\ref{MECases} if some $B_i$ spans in the middle of the data set, $P^t_L(B_i) \leq \frac{1}{2}$ and $P^t_R(B_i) \leq \frac{1}{2}$, we can still show that \textit{Case 1} occurs. Suppose for the remainder of the proof that there is no such middle-spanning $B_i$.

Let $(B_m, B_{m+1})$ be the unique middle gap (i.e. $P^t_L(B_m, B_{m+1}) < \frac{1}{2}$ and $P^t_R(B_m, B_{m+1}) < \frac{1}{2}$) when \textit{Case 2} or \textit{Case 3} occurs. When \textit{Case 2} occurs, we know that we could select a root from among the two keys outside of our middle spanning gap, and choose the one which is closer to the middle. Thus, we have that (using Lemma~\ref{entr2x}):
\begin{equation}\label{EQC2} 
E_t \geq H(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t}, \frac{1}{2} + \frac{1}{2} \frac{q_m}{p_t}) \geq 2(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t})=1-\frac{q_m}{p_t}.
\end{equation}
Note that by the definition of the \textit{Rule c)} of section~\ref{The Modified Entropy Rule}, when this occurs, $(B_m, B_{m+1})$ must be a leaf of depth at most 2. Thus, this condition can only happen twice for each $(B_m, B_{m+1})$ gap. \\

When neither \textit{Case 1} nor \textit{Case 2} occur (and we have a $(B_m, B_{m+1})$ spanning the middle) we must have \textit{Case 3}. This gives us
\begin{align*}
E_t \geq H(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t}, \frac{1}{2} + \frac{1}{2} \frac{q_m}{p_t}).
\end{align*}
We again apply Claim~\ref{Claim1} and get
\begin{equation}\label{EQC3}
E_t \geq 1- \frac{4}{5} (\frac{q_m}{p_t})^2.
\end{equation}

As in \cite{bayer1975improved} we define a $b_t$ for each subtree $t$ as follows. We want to have a value for $b_t$ such that $E_t \geq 1 - \frac{b_t}{p_t}$ in all cases. Using \textit{Cases 1,2}, and \textit{3} are their respective equations~\ref{EQC1},~\ref{EQC2}, and~\ref{EQC3} we do just that: \\
Let $b_t=2\cdot p_r$ when \textit{Case 1} occurs. $B_r$ is the root of $b_t$. \\
Let $b_t=2\cdot q_m$ when \textit{Case 2} occurs. $(B_m, B_{m+1})$ is middle gap of $b_t$. \\
Let $b_t=\frac{q_m^2}{p_t}$ when \textit{Case 3} occurs. $(B_m, B_{m+1})$ is middle gap of $b_t$. \\


Note that, in 1975 P. Bayer \cite{bayer1975improved} showed that the cost $C$ of our tree could be defined as (\textbf{Lemma 2.3}) \\
\begin{align*}
C = \sum_{t \in S_T} p_t
\end{align*}
and the entropy could be calculated by
\begin{align*}
H = \sum_{t \in S_T} p_t \cdot E_t
\end{align*}  
where $S_T$ is the set of all subtrees of our tree $T$. \\

\noindent Thus, by subbing in $E_t \geq 1 - \frac{b_t}{p_t}$ and rearranging we get: 
\begin{align*}
&H = \sum_{t \in S_T} p_t \cdot E_t \geq \sum_{t \in S_T} p_t - \sum_{t \in S_T} b_t = C - \sum_{t \in S_T} b_t \\
 \implies &C \leq H + \sum_{t \in S_T} b_t
\end{align*}
As mentioned above, \textit{Case 1} and \textit{Case 2} can only occur once and twice respectively for any potential root $B_r$ or gap $(B_m, B_{m+1})$. \textit{Case 3} however, can occur many times for a gap $(B_m, B_{m+1})$. Each time it occurs though, $\frac{q_m}{p_t}$ must increase by a factor of at least $\frac{5}{4}$ since $\max(P^t_L(B_r), P^t_R(p_r)) < \frac{4}{5}$ for the root $B_r$ of the subtree by \textit{Case 3)} of Lemma~\ref{MECases}. Moreover, if $\frac{q_m}{p_t} > \frac{1}{2}$ then we will have \textit{Case 2}. Let $S_m$ be the set of all subtrees $t$ for which $(B_m, B_{m+1})$ is the middle gap and \textit{Case 3} only applies. We have that
\begin{align*}
C \leq H + \sum_{t \in S_T} b_t = H + 2 \sum\limits_{r = 1}^n p_r + 2 \sum\limits_{m = 0}^n q_m + \sum\limits_{m = 0}^n \sum\limits_{t \in S_m} \frac{4}{5}\frac{q_m^2}{p_t}
\end{align*}
 \\ 
By factoring out $q_m$ and examining only cases up to $\frac{q_m}{p_t} = \frac{1}{2}$ (since otherwise \textit{Case 2} will occur) we get:
\begin{align*}
C &\leq H + 2 + \sum\limits_{m = 0}^n (\frac{4}{5} \cdot q_m) \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x \\
 &= H + 2 + \sum\limits_{m = 0}^n \frac{4}{5} \cdot q_m \cdot \frac{1}{2} \cdot (\frac{1}{1-\frac{4}{5}}) \text{    (geometric series)} \\
 &=H + 2 + 2\cdot \sum\limits_{m = 0}^n q_m \\
 &\leq H + 4.
\end{align*}

\end{proof}

\section{APPENDIX - TODO AS REAL APPENDIX}
\begin{claim}\label{Claim1}
$H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) \geq 1- \frac{4}{5} x^2$ when $0 < x < \frac{1}{2}$
\end{claim}
\begin{proof}
In order to prove the claim, we find the minimum of
\begin{align*}
H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} x^2) 
\end{align*}
when $0 < x < \frac{1}{2}$. To do this, we define $F(x)$ and take the derivative with respect to $x$.
\begin{align*}
F(x) &= H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) \\
F(x) &= - (\frac{1}{2}-\frac{1}{2} x)\cdot \lg(\frac{1}{2}-\frac{1}{2} x) - (\frac{1}{2} + \frac{1}{2} x)\cdot\lg(\frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} x^2) \\
\implies F'(x) &= \lg(\frac{1}{2}-\frac{1}{2} x) - \lg(\frac{1}{2} + \frac{1}{2} x) + \frac{8}{5}x \text{ (with some careful manipulation)}
\end{align*}

The only root occurs when $x = 0$. Thus, we check when $x \rightarrow 0$ and $x \rightarrow \frac{1}{2}$. We note that: \\
\begin{align*}
&F'(x) \xrightarrow{x \to 0} 0^{+}  \text{ and} \\
&F'(x) \xrightarrow{x \to \frac{1}{2}} 0.0112781 > 0
\end{align*}
Thus, $H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) > 0$ for $ 0 < x < \frac{1}{2}$ which proves the claim.
\end{proof} 

\section{Typesetting instructions -- please read carefully}
Please comply with the following instructions when preparing your article for a LIPIcs proceedings volume. 
\begin{itemize}
\item Use pdflatex and an up-to-date LaTeX system.
\item Use further LaTeX packages only if required. Avoid usage of packages like \verb+enumitem+, \verb+enumerate+, \verb+cleverref+. Keep it simple, i.e. use as few additional packages as possible.
\item Add custom made macros carefully and only those which are needed in the article (i.e., do not simply add your convolute of macros collected over the years).
\item Do not use a different main font. For example, the usage of the \verb+times+-package is forbidden.
\item Provide full author names (especially with regard to the first name) in the \verb+\author+ macro and in the \verb+\Copyright+ macro.
\item Fill out the \verb+\subjclass+ and \verb+\keywords+ macros. For the \verb+\subjclass+, please refer to the ACM classification at \url{http://www.acm.org/about/class/ccs98-html}.
\item Take care of suitable linebreaks and pagebreaks. No overfull \verb+\hboxes+ should occur in the warnings log.
\item Provide suitable graphics of at least 300dpi (preferrably in pdf format).
\item Use the provided sectioning macros: \verb+\section+, \verb+\subsection+, \verb+\subsection*+, \verb+\paragraph+, \verb+\subparagraph*+, ... ``Self-made'' sectioning commands (for example, \verb+\noindent{\bf My+ \verb+subparagraph.}+ will be removed and replaced by standard LIPIcs style sectioning commands.
\item Do not alter the spacing of the  \verb+lipics-v2016.cls+ style file. Such modifications will be removed.
\item Do not use conditional structures to include/exclude content. Instead, please provide only the content that should be published -- in one file -- and nothing else.
\item Remove all comments, especially avoid commenting large text blocks and using \verb+\iffalse+ $\ldots$ \verb+\fi+ constructions.
\item Keep the standard style (\verb+plainurl+) for the bibliography as provided by the\linebreak \verb+lipics-v2016.cls+ style file.
\item Use BibTex and provide exactly one BibTex file for your article. The BibTex file should contain only entries that are referenced in the article. Please make sure that there are no errors and warnings with the referenced BibTex entries.
\item Use a spellchecker to get rid of typos.
\item A manual for the LIPIcs style is available at \url{http://drops.dagstuhl.de/styles/lipics/lipics-v2016-authors/lipics-v2016-manual.pdf}.
\end{itemize}


\section{Lorem ipsum dolor sit amet}

Lorem ipsum dolor sit amet, consectetur adipiscing elit \cite{DBLP:journals/cacm/Knuth74}. Praesent convallis orci arcu, eu mollis dolor. Aliquam eleifend suscipit lacinia. Maecenas quam mi, porta ut lacinia sed, convallis ac dui. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse potenti. Donec eget odio et magna ullamcorper vehicula ut vitae libero. Maecenas lectus nulla, auctor nec varius ac, ultricies et turpis. Pellentesque id ante erat. In hac habitasse platea dictumst. Curabitur a scelerisque odio. Pellentesque elit risus, posuere quis elementum at, pellentesque ut diam. Quisque aliquam libero id mi imperdiet quis convallis turpis eleifend. 

\begin{lemma}[Lorem ipsum]
\label{lemma:lorem}
Vestibulum sodales dolor et dui cursus iaculis. Nullam ullamcorper purus vel turpis lobortis eu tempus lorem semper. Proin facilisis gravida rutrum. Etiam sed sollicitudin lorem. Proin pellentesque risus at elit hendrerit pharetra. Integer at turpis varius libero rhoncus fermentum vitae vitae metus.
\end{lemma}

\begin{proof}
Cras purus lorem, pulvinar et fermentum sagittis, suscipit quis magna.
\end{proof}

\begin{theorem}[Curabitur pulvinar, \cite{DBLP:books/mk/GrayR93}]
\label{theorem:curabitur}
Nam liber tempor cum soluta nobis eleifend option congue nihil imperdiet doming id quod mazim placerat facer possim assum. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.
\end{theorem}

\subsection{Curabitur dictum felis id sapien}

Curabitur dictum felis id sapien mollis ut venenatis tortor feugiat. Curabitur sed velit diam. Integer aliquam, nunc ac egestas lacinia, nibh est vehicula nibh, ac auctor velit tellus non arcu. Vestibulum lacinia ipsum vitae nisi ultrices eget gravida turpis laoreet. Duis rutrum dapibus ornare. Nulla vehicula vulputate iaculis. Proin a consequat neque. Donec ut rutrum urna. Morbi scelerisque turpis sed elit sagittis eu scelerisque quam condimentum. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aenean nec faucibus leo. Cras ut nisl odio, non tincidunt lorem. Integer purus ligula, venenatis et convallis lacinia, scelerisque at erat. Fusce risus libero, convallis at fermentum in, dignissim sed sem. Ut dapibus orci vitae nisl viverra nec adipiscing tortor condimentum \cite{DBLP:journals/cacm/Dijkstra68a}. Donec non suscipit lorem. Nam sit amet enim vitae nisl accumsan pretium. 

\begin{lstlisting}[caption={Useless code},label=list:8-6,captionpos=t,float,abovecaptionskip=-\medskipamount]
for i:=maxint to 0 do 
begin 
    j:=square(root(i));
end;
\end{lstlisting}

\subsection{Proin ac fermentum augue}

Proin ac fermentum augue. Nullam bibendum enim sollicitudin tellus egestas lacinia euismod orci mollis. Nulla facilisi. Vivamus volutpat venenatis sapien, vitae feugiat arcu fringilla ac. Mauris sapien tortor, sagittis eget auctor at, vulputate pharetra magna. Sed congue, dui nec vulputate convallis, sem nunc adipiscing dui, vel venenatis mauris sem in dui. Praesent a pretium quam. Mauris non mauris sit amet eros rutrum aliquam id ut sapien. Nulla aliquet fringilla sagittis. Pellentesque eu metus posuere nunc tincidunt dignissim in tempor dolor. Nulla cursus aliquet enim. Cras sapien risus, accumsan eu cursus ut, commodo vel velit. Praesent aliquet consectetur ligula, vitae iaculis ligula interdum vel. Integer faucibus faucibus felis. 

\begin{itemize}
\item Ut vitae diam augue. 
\item Integer lacus ante, pellentesque sed sollicitudin et, pulvinar adipiscing sem. 
\item Maecenas facilisis, leo quis tincidunt egestas, magna ipsum condimentum orci, vitae facilisis nibh turpis et elit. 
\end{itemize}

\section{Pellentesque quis tortor}

Nec urna malesuada sollicitudin. Nulla facilisi. Vivamus aliquam tempus ligula eget ornare. Praesent eget magna ut turpis mattis cursus. Aliquam vel condimentum orci. Nunc congue, libero in gravida convallis \cite{DBLP:conf/focs/HopcroftPV75}, orci nibh sodales quam, id egestas felis mi nec nisi. Suspendisse tincidunt, est ac vestibulum posuere, justo odio bibendum urna, rutrum bibendum dolor sem nec tellus. 

\begin{lemma} [Quisque blandit tempus nunc]
Sed interdum nisl pretium non. Mauris sodales consequat risus vel consectetur. Aliquam erat volutpat. Nunc sed sapien ligula. Proin faucibus sapien luctus nisl feugiat convallis faucibus elit cursus. Nunc vestibulum nunc ac massa pretium pharetra. Nulla facilisis turpis id augue venenatis blandit. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.
\end{lemma}

Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.


\subparagraph*{Acknowledgements.}

I want to thank \dots

\appendix
\section{Morbi eros magna}

Morbi eros magna, vestibulum non posuere non, porta eu quam. Maecenas vitae orci risus, eget imperdiet mauris. Donec massa mauris, pellentesque vel lobortis eu, molestie ac turpis. Sed condimentum convallis dolor, a dignissim est ultrices eu. Donec consectetur volutpat eros, et ornare dui ultricies id. Vivamus eu augue eget dolor euismod ultrices et sit amet nisi. Vivamus malesuada leo ac leo ullamcorper tempor. Donec justo mi, tempor vitae aliquet non, faucibus eu lacus. Donec dictum gravida neque, non porta turpis imperdiet eget. Curabitur quis euismod ligula. 


%%
%% Bibliography
%%

%% Either use bibtex (recommended), 

\bibliography{lipics-v2016-sample-article}

%% .. or use the thebibliography environment explicitely



\end{document}
